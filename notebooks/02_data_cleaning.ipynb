{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Data Cleaning Worflow\n",
    "# -----------------------------\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "PROJECT_ROOT = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "RAW_DATA_PATH = os.path.join(PROJECT_ROOT, \"data\", \"raw\")\n",
    "CLEANED_DATA_PATH = os.path.join(PROJECT_ROOT, \"data\", \"cleaned\")\n",
    "os.makedirs(CLEANED_DATA_PATH, exist_ok=True)\n",
    "\n",
    "log_file_path = os.path.join(CLEANED_DATA_PATH, \"cleaned_log.txt\")\n",
    "\n",
    "# Initialize log file\n",
    "with open(log_file_path, \"w\") as f:\n",
    "    f.write(f\"Provenance log started: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "\n",
    "def log_step(description):\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    log_entry = f\"{timestamp} - {description}\\n\"\n",
    "    with open(log_file_path, \"a\") as f:\n",
    "        f.write(log_entry)\n",
    "\n",
    "\n",
    "files = [\n",
    "    \"campaign_desc.csv\", \"campaign_table.csv\", \"coupon.csv\",\n",
    "    \"coupon_redempt.csv\", \"causal_data.csv\", \"product.csv\",\n",
    "    \"transaction_data.csv\", \"hh_demographic.csv\"\n",
    "]\n",
    "\n",
    "# -----------------------------\n",
    "# Load Raw Data\n",
    "# -----------------------------\n",
    "datasets = {}\n",
    "for f in files:\n",
    "    path = os.path.join(RAW_DATA_PATH, f)\n",
    "    df = pd.read_csv(path)\n",
    "    datasets[f] = df\n",
    "    log_step(f\"Loaded {f}, shape: {df.shape}\")\n",
    "\n",
    "# -----------------------------\n",
    "# Generic Cleaning Function\n",
    "# -----------------------------\n",
    "missing_placeholders = [\"Unknown\", \"None\", \"NONE\", \"None/Unknown\", \"\", \"U\", \"NaN\"]\n",
    "\n",
    "def clean_dataframe(df, df_name):\n",
    "    n_rows = len(df)\n",
    "\n",
    "    # Remove duplicates\n",
    "    dup_count = df.duplicated().sum()\n",
    "    if dup_count > 0:\n",
    "        df.drop_duplicates(inplace=True)\n",
    "        dup_per = (dup_count / n_rows) * 100\n",
    "        log_step(f\"Removed {dup_count} duplicate rows ({dup_per:.2f}%) from {df_name}.\")\n",
    "    else:\n",
    "        log_step(f\"No duplicates found in {df_name}.\")\n",
    "\n",
    "    # Standardize string columns\n",
    "    for col in df.select_dtypes(include='object').columns:\n",
    "        df[col] = df[col].str.strip()\n",
    "\n",
    "    # Identify missing or unknown values and fill for all columns\n",
    "    cols_with_missing = {}\n",
    "    for col in df.columns:\n",
    "        series_as_str = df[col].astype(str).str.strip()\n",
    "        count_na = df[col].isna().sum()\n",
    "        count_missing = series_as_str.isin(missing_placeholders).sum()\n",
    "        total_missing = count_na + count_missing\n",
    "        if total_missing > 0:\n",
    "            missing_per = (total_missing / n_rows) * 100\n",
    "            cols_with_missing[col] = {'missing': total_missing, 'perc': missing_per}\n",
    "            # Fill missing/unknown with 'Unknown' if object type\n",
    "            if df[col].dtype == 'object':\n",
    "                df[col] = df[col].replace(missing_placeholders, 'Unknown')\n",
    "\n",
    "    if cols_with_missing:\n",
    "        for col, counts in cols_with_missing.items():\n",
    "            log_step(f\"{df_name}: {col} -> Missing={counts['missing']}, Missing Per={counts['perc']}\")\n",
    "\n",
    "    return df\n",
    "\n",
    "# -----------------------------\n",
    "# Apply Cleaning\n",
    "# -----------------------------\n",
    "for fname, df in datasets.items():\n",
    "    datasets[fname] = clean_dataframe(df, fname)\n",
    "\n",
    "# -----------------------------\n",
    "# Filter invalid transactions (non-positive quantity or sales)\n",
    "# -----------------------------\n",
    "if \"transaction_data.csv\" in datasets:\n",
    "    transactions = datasets[\"transaction_data.csv\"]\n",
    "    initial_count = len(transactions)\n",
    "    transactions = transactions[(transactions['QUANTITY'] > 0) & (transactions['SALES_VALUE'] > 0)]\n",
    "    log_step(f\"Filtered {initial_count - len(transactions)} invalid transactions with non-positive quantity or sales value.\")\n",
    "    datasets[\"transaction_data.csv\"] = transactions\n",
    "\n",
    "# -----------------------------\n",
    "# Save Cleaned Datasets\n",
    "# -----------------------------\n",
    "for fname, df in datasets.items():\n",
    "    cleaned_file = os.path.join(CLEANED_DATA_PATH, f\"{fname.replace('.csv','')}_cleaned.csv\")\n",
    "    df.to_csv(cleaned_file, index=False)\n",
    "    log_step(f\"Saved cleaned dataset: {cleaned_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
