{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Customer Segmentation Processing\n",
    "# -----------------------------\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "PROJECT_ROOT = os.path.abspath(os.path.join(os.getcwd(), \"../..\")) \n",
    "\n",
    "# Paths to cleaned datasets\n",
    "CLEANED_DATA_DIR = os.path.join(PROJECT_ROOT, \"data\", \"cleaned\")\n",
    "PROCESSED_DATA_DIR = os.path.join(PROJECT_ROOT, \"data\", \"customer_segmentation\")\n",
    "os.makedirs(PROCESSED_DATA_DIR, exist_ok=True)\n",
    "\n",
    "# Initialize provenance log\n",
    "log_file_path = os.path.join(PROCESSED_DATA_DIR, \"provenance_log.txt\")\n",
    "\n",
    "with open(log_file_path, \"w\") as f:\n",
    "    f.write(f\"Provenance log started: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "\n",
    "def log_step(description):\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    log_entry = f\"{timestamp} - {description}\\n\"\n",
    "    with open(log_file_path, \"a\") as f:\n",
    "        f.write(log_entry)\n",
    "\n",
    "# -----------------------------\n",
    "# Load Cleaned Datasets\n",
    "# -----------------------------\n",
    "hh_demo = pd.read_csv(os.path.join(CLEANED_DATA_DIR, \"hh_demographic_cleaned.csv\"))\n",
    "transactions = pd.read_csv(os.path.join(CLEANED_DATA_DIR, \"transaction_data_cleaned.csv\"))\n",
    "products = pd.read_csv(os.path.join(CLEANED_DATA_DIR, \"product_cleaned.csv\"))\n",
    "campaign_desc = pd.read_csv(os.path.join(CLEANED_DATA_DIR, \"campaign_desc_cleaned.csv\"))\n",
    "coupon_redempt = pd.read_csv(os.path.join(CLEANED_DATA_DIR, \"coupon_redempt_cleaned.csv\"))\n",
    "\n",
    "log_step(\"Cleaned datasets loaded: hh_demo, transactions, products, coupon_redempt, campaign_desc.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Merge Datasets for Customer Segmentation\n",
    "# -----------------------------\n",
    "\n",
    "# Merge household demographics\n",
    "tx = transactions.merge(hh_demo, how='left', on='household_key')\n",
    "log_step(\"Merged household demographics into transactions.\")\n",
    "\n",
    "# Merge product info\n",
    "tx = tx.merge(products, how='left', on='PRODUCT_ID')\n",
    "log_step(\"Merged product info into transactions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Save Processed Dataset Ready for Analysis\n",
    "# -----------------------------\n",
    "PROCESSED_DATA_FILE = os.path.join(PROCESSED_DATA_DIR, \"transactions_merged.csv\")\n",
    "tx.to_csv(PROCESSED_DATA_FILE, index=False)\n",
    "\n",
    "log_step(f\"Saved merged transactions to {PROCESSED_DATA_FILE}. Shape: {tx.shape} with  Columns:  {list(tx.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Load and clean\n",
    "# -----------------------------\n",
    "\n",
    "tx = pd.read_csv(PROCESSED_DATA_FILE)\n",
    "log_step(f\"Merged Transactions loaded. Shape: {tx.shape} Columns: {tx.columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define columns to check\n",
    "critical_demo_cols = ['MARITAL_STATUS_CODE', 'INCOME_DESC']\n",
    "missing_placeholders = [\"Unknown\", \"None\", \"NONE\", \"None/Unknown\", \"\", \"NaN\"]\n",
    "\n",
    "# Count rows before dropping\n",
    "initial_rows = tx.shape[0]\n",
    "\n",
    "# Boolean mask for rows where either column is NaN or placeholder\n",
    "mask_missing = tx[critical_demo_cols].apply(\n",
    "    lambda col: col.isna() | col.astype(str).str.strip().isin(missing_placeholders)\n",
    ").any(axis=1)\n",
    "\n",
    "# Count & percentage\n",
    "num_dropped = mask_missing.sum()\n",
    "per_dropped = num_dropped / initial_rows * 100\n",
    "\n",
    "# Drop rows\n",
    "tx_clean = tx[~mask_missing].reset_index(drop=True)\n",
    "\n",
    "# Log step\n",
    "log_step(f\"Dropped {num_dropped} rows with missing/unknown MARITAL_STATUS_CODE or INCOME_DESC ({per_dropped:.2f}% of total).\")\n",
    "log_step(f\"Shape after drop: {tx_clean.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Total Spending per Household\n",
    "# -----------------------------\n",
    "\n",
    "log_step(\"Starting household-level aggregation...\")\n",
    "\n",
    "# Aggregate transactional data per household\n",
    "agg = (\n",
    "    tx.groupby(\"household_key\")\n",
    "    .agg({\n",
    "        \"SALES_VALUE\": \"sum\",\n",
    "        \"QUANTITY\": \"sum\",\n",
    "        \"BASKET_ID\": pd.Series.nunique,\n",
    "        \"COUPON_DISC\": lambda x: (x > 0).sum()\n",
    "    })\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Rename for clarity\n",
    "agg = agg.rename(columns={\n",
    "    \"SALES_VALUE\": \"total_spent\",\n",
    "    \"QUANTITY\": \"total_quantity\",\n",
    "    \"BASKET_ID\": \"num_transactions\",\n",
    "    \"COUPON_DISC\": \"num_coupons_redeemed\"\n",
    "})\n",
    "\n",
    "# Derived metrics\n",
    "agg[\"avg_basket_size\"] = agg[\"total_quantity\"] / agg[\"num_transactions\"]\n",
    "agg[\"coupon_redemption_rate\"] = agg[\"num_coupons_redeemed\"] / agg[\"num_transactions\"]\n",
    "\n",
    "log_step(f\"Aggregated {agg.shape[0]} households. Columns: {list(agg.columns)}\")\n",
    "\n",
    "# -----------------------------\n",
    "# Merge Demographic Information\n",
    "# -----------------------------\n",
    "\n",
    "demographics_cols = [\n",
    "    \"household_key\", \"AGE_DESC\", \"MARITAL_STATUS_CODE\", \"INCOME_DESC\",\n",
    "    \"HOMEOWNER_DESC\", \"HH_COMP_DESC\", \"HOUSEHOLD_SIZE_DESC\", \"KID_CATEGORY_DESC\"\n",
    "]\n",
    "\n",
    "agg = agg.merge(tx[demographics_cols].drop_duplicates(), on=\"household_key\", how=\"left\")\n",
    "log_step(f\"Merged demographic data. Shape after merge: {agg.shape}\")\n",
    "\n",
    "# -----------------------------\n",
    "# Drop Missing/Unknown Demographics\n",
    "# -----------------------------\n",
    "\n",
    "missing_placeholders = [\"Unknown\", \"NULL\", \"nan\", \"None\", \"NA\", \"Not Available\", \" \"]\n",
    "\n",
    "mask_missing_all = agg[demographics_cols[1:]].apply(\n",
    "    lambda col: col.isna() | col.astype(str).str.strip().isin(missing_placeholders)\n",
    ").any(axis=1)\n",
    "\n",
    "num_dropped = mask_missing_all.sum()\n",
    "per_dropped = num_dropped / agg.shape[0] * 100\n",
    "\n",
    "agg = agg[~mask_missing_all].reset_index(drop=True)\n",
    "\n",
    "log_step(f\"Dropped {num_dropped} households with missing/unknown demographic values ({per_dropped:.2f}%).\")\n",
    "log_step(f\"Final aggregated dataset shape: {agg.shape}\")\n",
    "\n",
    "\n",
    "log_step(\"Sample of aggregated data:\")\n",
    "log_step(agg.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Demographic Distribution - Pie Charts (Saved)\n",
    "# -----------------------------\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Create directory for figures\n",
    "FIGURES_DIR = os.path.join(PROJECT_ROOT, \"data\", \"customer_segmentation\", \"figures\")\n",
    "os.makedirs(FIGURES_DIR, exist_ok=True)\n",
    "\n",
    "log_step(\"Starting demographic distribution pie chart generation...\")\n",
    "\n",
    "# List of demographic columns to visualize\n",
    "demo_cols = [\n",
    "    'AGE_DESC', 'MARITAL_STATUS_CODE', 'INCOME_DESC',\n",
    "    'HOMEOWNER_DESC', 'HH_COMP_DESC', 'HOUSEHOLD_SIZE_DESC', 'KID_CATEGORY_DESC'\n",
    "]\n",
    "\n",
    "# Function to generate and save pie chart for a column\n",
    "def save_pie_chart(col_name):\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    data = agg[col_name].value_counts(dropna=False)\n",
    "    labels = data.index\n",
    "    sizes = data.values\n",
    "\n",
    "    plt.pie(sizes, labels=labels, autopct='%1.1f%%', startangle=140)\n",
    "    plt.title(f\"Distribution of {col_name.replace('_', ' ')}\", fontsize=12)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save figure\n",
    "    fig_path = os.path.join(FIGURES_DIR, f\"{col_name.lower()}_piechart.png\")\n",
    "    plt.savefig(fig_path)\n",
    "    plt.close()\n",
    "\n",
    "    log_step(f\"Saved pie chart for {col_name} to {fig_path}\")\n",
    "\n",
    "# Generate and save pie charts for each demographic column\n",
    "for col in demo_cols:\n",
    "    save_pie_chart(col)\n",
    "\n",
    "log_step(f\"Completed saving all demographic pie charts to {FIGURES_DIR}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
